#!/usr/bin/env python3
"""
Flex HRM Knowledge Base Scraper - Enhanced Version
Skrapar alla artiklar med rik metadata, kategorisering och taggning
"""

import requests
from bs4 import BeautifulSoup
import time
import os
import re
import json
from urllib.parse import unquote, urljoin, urlparse
from collections import defaultdict
import hashlib

# Kategorier att skrapa
CATEGORIES = [
    {'name': 'Systemgemensamt', 'url': 'https://knowledge.flexapplications.se/systemgemensamt', 'folder': 'systemgemensamt'},
    {'name': 'Time', 'url': 'https://knowledge.flexapplications.se/time', 'folder': 'time'},
    {'name': 'Employee', 'url': 'https://knowledge.flexapplications.se/employee', 'folder': 'employee'},
    {'name': 'Travel & Expense', 'url': 'https://knowledge.flexapplications.se/travel-expense', 'folder': 'travel-expense'},
    {'name': 'Payroll', 'url': 'https://knowledge.flexapplications.se/payroll', 'folder': 'payroll'},
    {'name': 'Plan', 'url': 'https://knowledge.flexapplications.se/plan', 'folder': 'plan'}
]

def create_slug(title: str) -> str:
    """Skapar en slug fr√•n titel"""
    slug = title.lower()
    slug = slug.replace('‚öôÔ∏è', '')
    slug = slug.replace('√•', 'a').replace('√§', 'a').replace('√∂', 'o')
    slug = re.sub(r'[^a-z0-9]+', '-', slug)
    slug = slug.strip('-')
    return slug[:100]

def classify_article_type(title: str, content: str) -> str:
    """Klassificerar artikeltyp baserat p√• titel och inneh√•ll"""
    title_lower = title.lower()
    
    # Begrepp/glossary artiklar
    if 'begrepp' in title_lower or 'vad betyder' in title_lower or 'vad √§r' in title_lower:
        return 'concept'
    
    # Hur-g√∂r-jag (howto) artiklar
    if title_lower.startswith('hur ') or 'hur g√∂r' in title_lower or 'hur skapar' in title_lower:
        return 'howto'
    
    # Konfigurations artiklar (har ofta ‚öôÔ∏è)
    if '‚öôÔ∏è' in title or 'inst√§llningar' in title_lower or 'st√§ller in' in title_lower:
        return 'config'
    
    # FAQ artiklar
    if title_lower.startswith('kan ') or title_lower.startswith('g√•r det'):
        return 'faq'
    
    # Funktionsbeskrivningar
    if 'fungerar' in title_lower or 'anv√§nds' in title_lower:
        return 'feature'
    
    return 'other'

def extract_tags(title: str, content: str, category: str) -> list:
    """Extraherar relevanta tags fr√•n titel och inneh√•ll"""
    tags = set()
    
    # Vanliga nyckelord per kategori
    keywords_map = {
        'time': ['tidrapport', 'st√§mpling', 'schema', 'fr√•nvaro', 'semester', '√∂vertid', 
                 'ob', 'l√∂n', 'mobil', 'attestering', 'saldo', 'tidkod', 'beredskap'],
        'employee': ['anst√§llning', 'dokument', 'signering', 'medarbetarsamtal', 'kompetens', 
                     'kurs', 'onboarding', 'offboarding', 'l√∂nerevision', 'cv'],
        'payroll': ['l√∂n', 'semester', 'skatt', 'agi', 'pension', 'bokf√∂ring', 'l√∂neart'],
        'travel-expense': ['resa', 'traktamente', 'utl√§gg', 'bil', 'kvitto', 'attestering'],
        'systemgemensamt': ['anv√§ndare', 'beh√∂righet', 'roll', 's√§kerhet', 'integration', 'mobil'],
        'plan': ['schema', 'kalender', 'prenumeration']
    }
    
    # S√∂k efter relevanta keywords
    text = (title + ' ' + content).lower()
    category_keywords = keywords_map.get(category, [])
    
    for keyword in category_keywords:
        if keyword in text:
            tags.add(keyword)
    
    # Extrahera specifika begrepp fr√•n titel
    if 'mobil' in title.lower():
        tags.add('mobil')
    if 'hrm time' in title.lower():
        tags.add('hrm-time')
    if 'hrm employee' in title.lower():
        tags.add('hrm-employee')
    
    return sorted(list(tags))[:8]  # Max 8 tags

def determine_difficulty(title: str, content: str, article_type: str) -> str:
    """Best√§mmer sv√•righetsgrad"""
    content_lower = content.lower()
    title_lower = title.lower()
    
    # Avancerade indikatorer
    advanced_indicators = ['formel', 'integration', 'api', 'regelverk', 'ber√§kning', 'koncern']
    if any(ind in content_lower or ind in title_lower for ind in advanced_indicators):
        return 'advanced'
    
    # Konfig √§r oftast intermediate
    if article_type == 'config':
        return 'intermediate'
    
    # Begrepp √§r oftast beginner
    if article_type == 'concept':
        return 'beginner'
    
    # Default till beginner f√∂r korta, enkla artiklar
    if len(content) < 500:
        return 'beginner'
    
    return 'intermediate'

def categorize_subcategory(title: str, content: str, category: str, tags: list) -> str:
    """Kategoriserar artikel i underkategori"""
    title_lower = title.lower()
    content_lower = content.lower()
    
    subcategory_rules = {
        'time': {
            'Mobil & St√§mpling': ['mobil', 'st√§mpl', 'timeclock', 'st√§mpelklocka'],
            'Tidrapportering': ['tidrapport', 'dagredovisning', 'periodredovisning', 'tidkod'],
            'Fr√•nvaro & Semester': ['fr√•nvaro', 'semester', 'sjuk', 'ledighet', 'vab'],
            'Inst√§llningar': ['‚öôÔ∏è', 'inst√§llning', 'st√§ller in', 'konfigurer'],
            'Schema & Planering': ['schema', 'dagschema', 'planering', 'schemastartdatum'],
            'L√∂n & √ñverf√∂ring': ['√∂verf√∂r', 'l√∂n', 'l√∂nesystem'],
            '√ñvertid & Ers√§ttning': ['√∂vertid', 'ob', 'ers√§ttning', 'beredskap'],
            'Attestering & Granskning': ['attestering', 'granskning', 'delattestering'],
            'Saldon & Ber√§kning': ['saldo', 'ber√§kning', 'h√§ndelse', 'formel']
        },
        'employee': {
            'Anst√§llningshantering': ['anst√§llning', 'personaldata', 'anst√§lld', 'las'],
            'Dokument & E-signering': ['dokument', 'signering', 'scrive', 'verified'],
            'Medarbetarsamtal': ['medarbetarsamtal', 'performance', 'samtalsmallar'],
            'Kompetens & Kurser': ['kompetens', 'kurs', 'utbildning'],
            'Onboarding & Offboarding': ['onboarding', 'offboarding', 'nyanst√§llning'],
            'L√∂nerevision': ['l√∂nerevision', 'l√∂ne√∂versyn', 'l√∂nepott'],
            'Organisation': ['organisation', 'organisationstr√§d', 'chef'],
            'Integration': ['integration', 'teamtailor', 'winningtemp', 'bky']
        },
        'payroll': {
            'L√∂neberedning': ['l√∂neberedning', 'l√∂nek√∂rning', 'l√∂neunderlag'],
            'Semesterhantering': ['semester', 'semesterl√∂n', 'semestergrundande'],
            'Skatt & AGI': ['skatt', 'agi', 'arbetsgivaravgift'],
            'Pension': ['pension', 'tj√§nstepension'],
            'Bokf√∂ring': ['bokf√∂ring', 'kontering', 'verifikation'],
            'L√∂nearter': ['l√∂neart', 'l√∂nekod'],
            'Inst√§llningar': ['‚öôÔ∏è', 'inst√§llning']
        },
        'travel-expense': {
            'Reser√§kningar': ['reser√§kning', 'resa'],
            'Traktamente': ['traktamente', 'trakta'],
            'Utl√§gg & Kvitton': ['utl√§gg', 'kvitto', 'bilaga'],
            'Bilresor': ['bil', 'milers√§ttning'],
            'Attestering': ['attestering', 'granskning'],
            'Inst√§llningar': ['‚öôÔ∏è', 'inst√§llning']
        },
        'systemgemensamt': {
            'Anv√§ndare & Beh√∂righet': ['anv√§ndare', 'beh√∂righet', 'roll', 's√§kerhet'],
            'Mobil': ['mobil', 'mobile', 'app'],
            'Integration': ['integration', 'api', 'import', 'export'],
            'Rapporter': ['rapport', 'rapportgenerator'],
            'Register': ['register', 'projekt', 'kund', 'kontering'],
            'Inst√§llningar': ['‚öôÔ∏è', 'inst√§llning']
        },
        'plan': {
            'Schemal√§ggning': ['schema', 'planering'],
            'Kalender': ['kalender', 'prenumeration'],
            'Begrepp': ['begrepp'],
            'Inst√§llningar': ['‚öôÔ∏è', 'inst√§llning']
        }
    }
    
    rules = subcategory_rules.get(category, {})
    
    # R√§kna matchningar f√∂r varje underkategori
    matches = defaultdict(int)
    for subcat, keywords in rules.items():
        for keyword in keywords:
            if keyword in title_lower or keyword in content_lower:
                matches[subcat] += 1
    
    # Returnera b√§sta matchning
    if matches:
        return max(matches, key=matches.get)
    
    return '√ñvrigt'

def get_article_links(category_url: str) -> list:
    """H√§mtar alla artikell√§nkar fr√•n en kategorisida"""
    print(f"  H√§mtar artiklar fr√•n {category_url}...")
    response = requests.get(category_url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    article_links = []
    for link in soup.find_all('a', class_='hs-kb-category-article-list__link'):
        title = link.get_text(strip=True)
        url = link.get('href')
        article_links.append({'title': title, 'url': url})
    
    print(f"  ‚úì Hittade {len(article_links)} artiklar")
    return article_links

def download_image(img_url: str, article_slug: str, folder: str) -> str:
    """Laddar ner en bild och returnerar lokal s√∂kv√§g"""
    try:
        # Skapa images-mapp
        images_dir = f"documentation/{folder}/images"
        os.makedirs(images_dir, exist_ok=True)
        
        # Skapa unikt filnamn baserat p√• URL
        url_hash = hashlib.md5(img_url.encode()).hexdigest()[:8]
        ext = os.path.splitext(urlparse(img_url).path)[1] or '.jpg'
        filename = f"{article_slug}_{url_hash}{ext}"
        filepath = os.path.join(images_dir, filename)
        
        # Ladda ner om den inte redan finns
        if not os.path.exists(filepath):
            response = requests.get(img_url, timeout=10)
            if response.status_code == 200:
                with open(filepath, 'wb') as f:
                    f.write(response.content)
        
        # Returnera relativ s√∂kv√§g
        return f"images/{filename}"
    except Exception as e:
        print(f"\n    ‚ö†Ô∏è  Kunde inte ladda ner bild: {str(e)}")
        return None

def scrape_article(url: str) -> dict:
    """Skrapar inneh√•ll fr√•n en artikel"""
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # H√§mta titel
    title_elem = soup.find('h1')
    title = title_elem.get_text(strip=True) if title_elem else 'Ingen titel'
    
    # H√§mta artikel
    article = soup.find('article')
    if not article:
        return None
    
    # Samla alla bilder innan vi tar bort element
    images = []
    for img in article.find_all('img'):
        img_url = img.get('src') or img.get('data-src')
        if img_url:
            # Konvertera till absolut URL
            if not img_url.startswith('http'):
                img_url = urljoin(url, img_url)
            
            # Spara bildinformation
            images.append({
                'url': img_url,
                'alt': img.get('alt', ''),
                'title': img.get('title', '')
            })
    
    # Ta bort navigering, sidebar, feedback
    for elem in article.find_all(['aside', 'nav']):
        elem.decompose()
    for elem in article.find_all(class_=['hs-kb-sidebar', 'hs-kb-breadcrumbs', 'hs-kb-social_follow']):
        elem.decompose()
    for elem in article.find_all(string=lambda text: text and 'Hj√§lpte artikeln' in text):
        if elem.parent:
            elem.parent.decompose()
    
    # H√§mta HTML-inneh√•ll f√∂r bilder
    html_content = str(article)
    
    # H√§mta text
    text = article.get_text(separator='\n', strip=True)
    
    # Rensa bort √∂verdrivet whitespace
    text = re.sub(r'\n\n\n+', '\n\n', text)
    
    # Hitta datum
    date_match = re.search(r'den \d+ \w+ \d{4}', text)
    date = date_match.group(0) if date_match else ''
    
    # Ta bort datum fr√•n text
    if date:
        text = text.replace(date, '', 1).strip()
    
    # Ta bort titel fr√•n b√∂rjan om den upprepas
    if text.startswith(title):
        text = text[len(title):].strip()
    
    return {
        'title': title,
        'date': date,
        'content': text,
        'html_content': html_content,
        'images': images,
        'url': url
    }

def save_article_enhanced(article_data: dict, folder: str, category_name: str) -> tuple:
    """Sparar artikel som markdown och returnerar metadata"""
    slug = create_slug(article_data['title'])
    filepath = f"documentation/{folder}/{slug}.md"
    
    # Skapa mapp om den inte finns
    os.makedirs(f"documentation/{folder}", exist_ok=True)
    
    # Klassificera artikel
    article_type = classify_article_type(article_data['title'], article_data['content'])
    tags = extract_tags(article_data['title'], article_data['content'], folder)
    difficulty = determine_difficulty(article_data['title'], article_data['content'], article_type)
    subcategory = categorize_subcategory(article_data['title'], article_data['content'], folder, tags)
    is_config = '‚öôÔ∏è' in article_data['title']
    
    # Ladda ner bilder och skapa markdown-referenser
    image_references = []
    downloaded_images = []
    
    for i, img_data in enumerate(article_data.get('images', []), 1):
        local_path = download_image(img_data['url'], slug, folder)
        if local_path:
            downloaded_images.append(local_path)
            alt_text = img_data.get('alt', f'Bild {i}')
            image_references.append(f"\n![{alt_text}]({local_path})\n")
    
    # L√§gg till bilder i inneh√•llet
    images_section = ""
    if image_references:
        images_section = "\n\n## üì∏ Bilder\n" + "\n".join(image_references)
    
    # Skapa markdown-inneh√•ll
    md_content = f"""# {article_data['title']}

**Datum:** {article_data['date']}  
**Kategori:** {category_name}  
**Underkategori:** {subcategory}  
**Typ:** {article_type}  
**Sv√•righetsgrad:** {difficulty}  
**Tags:** {', '.join(tags) if tags else 'Ingen'}  
**Antal bilder:** {len(downloaded_images)}  
**URL:** {article_data['url']}

---

{article_data['content']}{images_section}
"""
    
    # Spara fil
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    # Spara ocks√• HTML-version f√∂r b√§ttre bildhantering
    html_filepath = f"documentation/{folder}/{slug}.html"
    with open(html_filepath, 'w', encoding='utf-8') as f:
        f.write(article_data.get('html_content', ''))
    
    # Returnera metadata f√∂r index
    metadata = {
        'title': article_data['title'],
        'file': f"{slug}.md",
        'htmlFile': f"{slug}.html",
        'slug': slug,
        'category': folder,
        'subcategory': subcategory,
        'type': article_type,
        'difficulty': difficulty,
        'tags': tags,
        'date': article_data['date'],
        'url': article_data['url'],
        'isConfig': is_config,
        'images': downloaded_images,
        'imageCount': len(downloaded_images),
        'excerpt': article_data['content'][:200] + '...' if len(article_data['content']) > 200 else article_data['content']
    }
    
    return filepath, metadata

def main():
    """Huvudfunktion"""
    print("\n" + "="*60)
    print("  FLEX HRM KNOWLEDGE BASE SCRAPER - ENHANCED")
    print("="*60 + "\n")
    
    total_articles = 0
    scraped_articles = 0
    all_metadata = {}
    
    for category in CATEGORIES:
        print(f"\nüìÇ Kategori: {category['name']}")
        print("-" * 60)
        
        category_metadata = []
        
        # H√§mta alla artikell√§nkar
        article_links = get_article_links(category['url'])
        total_articles += len(article_links)
        
        # Skrapa varje artikel
        for i, article_link in enumerate(article_links, 1):
            try:
                print(f"  [{i}/{len(article_links)}] {article_link['title'][:50]}...", end='')
                
                # Skrapa artikel
                article_data = scrape_article(article_link['url'])
                
                if article_data:
                    # Spara artikel och f√• metadata
                    filepath, metadata = save_article_enhanced(article_data, category['folder'], category['name'])
                    category_metadata.append(metadata)
                    scraped_articles += 1
                    print(f" ‚úì")
                else:
                    print(f" ‚úó (ingen data)")
                
                # V√§nta lite f√∂r att inte √∂verbelasta servern
                time.sleep(0.5)
                
            except Exception as e:
                print(f" ‚úó Fel: {str(e)}")
        
        # Spara index.json f√∂r kategorin
        index_path = f"documentation/{category['folder']}/index.json"
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(category_metadata, f, ensure_ascii=False, indent=2)
        
        all_metadata[category['folder']] = category_metadata
        print(f"\n‚úì Klar med {category['name']}: {len(category_metadata)} artiklar sparade")
    
    # Spara master index
    master_index = {
        'total_articles': scraped_articles,
        'total_categories': len(CATEGORIES),
        'last_updated': time.strftime('%Y-%m-%d %H:%M:%S'),
        'categories': {}
    }
    
    for cat in CATEGORIES:
        folder = cat['folder']
        if folder in all_metadata:
            # R√§kna underkategorier
            subcats = defaultdict(int)
            for article in all_metadata[folder]:
                subcats[article['subcategory']] += 1
            
            master_index['categories'][folder] = {
                'name': cat['name'],
                'total': len(all_metadata[folder]),
                'subcategories': dict(subcats)
            }
    
    with open('documentation/master_index.json', 'w', encoding='utf-8') as f:
        json.dump(master_index, f, ensure_ascii=False, indent=2)
    
    print("\n" + "="*60)
    print(f"‚úÖ KLART! Skrapade {scraped_articles}/{total_articles} artiklar")
    print(f"üìÅ Artiklar sparade i: documentation/")
    print(f"üìä Master index: documentation/master_index.json")
    print("="*60 + "\n")

if __name__ == "__main__":
    main()

