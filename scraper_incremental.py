#!/usr/bin/env python3
"""
Inkrementell Scraper f√∂r Flex HRM Documentation
Skrapar bara nya, √§ndrade eller raderade artiklar

Anv√§ndning:
    python3 scraper_incremental.py
"""

import requests
from bs4 import BeautifulSoup
import time
import os
import re
import json
from pathlib import Path
from datetime import datetime

# Kategorier
CATEGORIES = [
    {'name': 'Systemgemensamt', 'url': 'https://knowledge.flexapplications.se/systemgemensamt', 'folder': 'systemgemensamt'},
    {'name': 'Time', 'url': 'https://knowledge.flexapplications.se/time', 'folder': 'time'},
    {'name': 'Employee', 'url': 'https://knowledge.flexapplications.se/employee', 'folder': 'employee'},
    {'name': 'Travel & Expense', 'url': 'https://knowledge.flexapplications.se/travel-expense', 'folder': 'travel-expense'},
    {'name': 'Payroll', 'url': 'https://knowledge.flexapplications.se/payroll', 'folder': 'payroll'},
    {'name': 'Plan', 'url': 'https://knowledge.flexapplications.se/plan', 'folder': 'plan'}
]

def create_slug(title):
    """Skapar en slug fr√•n titel"""
    slug = title.lower()
    slug = slug.replace('‚öôÔ∏è', '')
    slug = slug.replace('√•', 'a').replace('√§', 'a').replace('√∂', 'o')
    slug = re.sub(r'[^a-z0-9]+', '-', slug)
    slug = slug.strip('-')
    return slug[:100]

def load_existing_articles(folder):
    """Ladda befintliga artiklar fr√•n index.json"""
    index_file = f"documentation/{folder}/index.json"
    
    if os.path.exists(index_file):
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                articles = json.load(f)
                # Skapa lookup dict med slug som nyckel
                return {article['slug']: article for article in articles}
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Kunde inte l√§sa {index_file}: {e}")
    
    return {}

def get_article_metadata_from_source(category_url):
    """
    H√§mta artikellista fr√•n k√§llan med titel, URL och synligt datum
    Skrapar INTE hela artikeln √§nnu, bara metadata
    """
    print(f"  H√§mtar artikellista fr√•n {category_url}...")
    
    try:
        response = requests.get(category_url, timeout=30)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles_metadata = []
        
        for link in soup.find_all('a', class_='hs-kb-category-article-list__link'):
            title = link.get_text(strip=True)
            url = link.get('href')
            slug = create_slug(title)
            
            articles_metadata.append({
                'title': title,
                'url': url,
                'slug': slug
            })
        
        print(f"  ‚úì Hittade {len(articles_metadata)} artiklar i listan")
        return articles_metadata
        
    except Exception as e:
        print(f"  ‚ùå Fel vid h√§mtning av artikellista: {e}")
        return []

def scrape_full_article(url):
    """Skrapar fullst√§ndig artikel fr√•n URL"""
    try:
        response = requests.get(url, timeout=30)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # H√§mta titel
        title_elem = soup.find('h1')
        title = title_elem.get_text(strip=True) if title_elem else 'Ingen titel'
        
        # H√§mta artikel
        article = soup.find('article')
        if not article:
            return None
        
        # Ta bort navigering, sidebar, feedback
        for elem in article.find_all(['aside', 'nav']):
            elem.decompose()
        for elem in article.find_all(class_=['hs-kb-sidebar', 'hs-kb-breadcrumbs', 'hs-kb-social_follow']):
            elem.decompose()
        for elem in article.find_all(string=lambda text: text and 'Hj√§lpte artikeln' in text):
            if elem.parent:
                elem.parent.decompose()
        
        # H√§mta text
        text = article.get_text(separator='\n', strip=True)
        
        # Rensa whitespace
        text = re.sub(r'\n\n\n+', '\n\n', text)
        
        # Hitta datum
        date_match = re.search(r'den \d+ \w+ \d{4}', text)
        date = date_match.group(0) if date_match else ''
        
        # Ta bort datum fr√•n text
        if date:
            text = text.replace(date, '', 1).strip()
        
        # Ta bort titel fr√•n b√∂rjan om den upprepas
        if text.startswith(title):
            text = text[len(title):].strip()
        
        return {
            'title': title,
            'date': date,
            'content': text,
            'url': url
        }
    except Exception as e:
        print(f"  ‚ùå Fel vid scraping av {url}: {e}")
        return None

def incremental_scrape():
    """
    Inkrementell scraping - bara nya, √§ndrade och raderade
    
    Returns:
        dict: Statistik √∂ver √§ndringar
    """
    print("\nüîÑ INKREMENTELL SCRAPING STARTAR")
    print("="*70)
    
    stats = {
        'new': 0,
        'updated': 0,
        'deleted': 0,
        'unchanged': 0,
        'errors': 0
    }
    
    for category in CATEGORIES:
        print(f"\nüìÇ Kategori: {category['name']}")
        folder = category['folder']
        
        # 1. Ladda befintliga artiklar
        existing_articles = load_existing_articles(folder)
        print(f"  üìÑ Befintliga artiklar: {len(existing_articles)}")
        
        # 2. H√§mta artikellista fr√•n k√§llan
        source_articles = get_article_metadata_from_source(category['url'])
        
        if not source_articles:
            print(f"  ‚ö†Ô∏è  Kunde inte h√§mta artiklar fr√•n k√§llan, hoppar √∂ver kategori")
            continue
        
        # 3. Skapa lookup f√∂r snabb s√∂kning
        source_slugs = {article['slug']: article for article in source_articles}
        
        # 4. Hitta nya och uppdaterade artiklar
        for source_article in source_articles:
            slug = source_article['slug']
            
            if slug not in existing_articles:
                # NY ARTIKEL
                print(f"  üÜï NY: {source_article['title'][:60]}")
                
                # Skrapa hela artikeln
                full_article = scrape_full_article(source_article['url'])
                
                if full_article:
                    # TODO: Spara artikel (anv√§nd befintlig save_article funktion)
                    stats['new'] += 1
                else:
                    stats['errors'] += 1
                
                time.sleep(1)  # Rate limiting
                
            else:
                # Artikel finns - kontrollera om uppdaterad
                existing = existing_articles[slug]
                
                # Enkel check: om titel √§ndrats
                if existing['title'] != source_article['title']:
                    print(f"  ‚úèÔ∏è  UPPDATERAD: {source_article['title'][:60]}")
                    
                    # Skrapa hela artikeln
                    full_article = scrape_full_article(source_article['url'])
                    
                    if full_article:
                        # J√§mf√∂r datum f√∂r att s√§kerst√§lla att det verkligen √§ndrats
                        if full_article['date'] != existing.get('date'):
                            stats['updated'] += 1
                            # TODO: Uppdatera artikel
                        else:
                            stats['unchanged'] += 1
                    else:
                        stats['errors'] += 1
                    
                    time.sleep(1)
                else:
                    # Of√∂r√§ndrad
                    stats['unchanged'] += 1
        
        # 5. Hitta raderade artiklar
        for slug in existing_articles:
            if slug not in source_slugs:
                article = existing_articles[slug]
                print(f"  ‚ùå RADERAD: {article['title'][:60]}")
                stats['deleted'] += 1
                # TODO: Ta bort artikel-fil och ta bort fr√•n index
    
    # Sammanfattning
    print("\n" + "="*70)
    print("üìä SAMMANFATTNING")
    print("="*70)
    print(f"  üÜï Nya artiklar:        {stats['new']}")
    print(f"  ‚úèÔ∏è  Uppdaterade artiklar: {stats['updated']}")
    print(f"  ‚ùå Raderade artiklar:    {stats['deleted']}")
    print(f"  ‚úì  Of√∂r√§ndrade artiklar: {stats['unchanged']}")
    print(f"  ‚ö†Ô∏è  Fel:                 {stats['errors']}")
    print("="*70)
    
    # Output f√∂r parsing i API
    total_changes = stats['new'] + stats['updated'] + stats['deleted']
    if total_changes > 0:
        parts = []
        if stats['new'] > 0:
            parts.append(f"{stats['new']} nya")
        if stats['updated'] > 0:
            parts.append(f"{stats['updated']} uppdaterade")
        if stats['deleted'] > 0:
            parts.append(f"{stats['deleted']} raderade")
        print(f"\n‚úì {', '.join(parts)}")
    else:
        print("\n‚ÑπÔ∏è  Inga √§ndringar detekterades")
    
    return stats

if __name__ == '__main__':
    import sys
    
    # Kolla om --incremental flagga finns
    if '--incremental' in sys.argv or '-i' in sys.argv:
        stats = incremental_scrape()
        
        # Exit code baserat p√• resultat
        if stats['errors'] > 0:
            sys.exit(1)  # Fel uppstod
        else:
            sys.exit(0)  # Success
    else:
        print("‚ö†Ô∏è  Denna scraper k√∂r endast i inkrementellt l√§ge")
        print("Anv√§ndning: python3 scraper_incremental.py --incremental")
        print("\nF√∂r full scraping, anv√§nd: python3 scraper.py")
        sys.exit(1)

