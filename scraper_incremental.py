#!/usr/bin/env python3
"""
Inkrementell Scraper f√∂r Flex HRM Documentation
Skrapar bara nya, √§ndrade eller raderade artiklar

Anv√§ndning:
    python3 scraper_incremental.py
"""

import requests
from bs4 import BeautifulSoup
import time
import os
import re
import json
import logging
from pathlib import Path
from datetime import datetime

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Custom headers - VIKTIGT f√∂r etisk scraping!
HEADERS = {
    'User-Agent': 'AxelssonDocBot/1.0 (github.com/pontush81/axelsson2; pontus.horberg@example.com)',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'sv-SE,sv;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
    'DNT': '1',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}

# Kategorier
CATEGORIES = [
    {'name': 'Systemgemensamt', 'url': 'https://knowledge.flexapplications.se/systemgemensamt', 'folder': 'systemgemensamt'},
    {'name': 'Time', 'url': 'https://knowledge.flexapplications.se/time', 'folder': 'time'},
    {'name': 'Employee', 'url': 'https://knowledge.flexapplications.se/employee', 'folder': 'employee'},
    {'name': 'Travel & Expense', 'url': 'https://knowledge.flexapplications.se/travel-expense', 'folder': 'travel-expense'},
    {'name': 'Payroll', 'url': 'https://knowledge.flexapplications.se/payroll', 'folder': 'payroll'},
    {'name': 'Plan', 'url': 'https://knowledge.flexapplications.se/plan', 'folder': 'plan'}
]

def create_slug(title):
    """Skapar en slug fr√•n titel"""
    slug = title.lower()
    slug = slug.replace('‚öôÔ∏è', '')
    slug = slug.replace('√•', 'a').replace('√§', 'a').replace('√∂', 'o')
    slug = re.sub(r'[^a-z0-9]+', '-', slug)
    slug = slug.strip('-')
    return slug[:100]

def load_existing_articles(folder):
    """Ladda befintliga artiklar fr√•n index.json"""
    index_file = f"documentation/{folder}/index.json"
    
    if os.path.exists(index_file):
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                articles = json.load(f)
                # Skapa lookup dict med slug som nyckel
                return {article['slug']: article for article in articles}
        except Exception as e:
            logger.warning(f"  ‚ö†Ô∏è  Kunde inte l√§sa {index_file}: {e}")
    
    return {}

def get_article_metadata_from_source(category_url):
    """
    H√§mta artikellista fr√•n k√§llan med titel, URL och synligt datum
    Skrapar INTE hela artikeln √§nnu, bara metadata
    """
    logger.info(f"H√§mtar artikellista fr√•n {category_url}")
    
    try:
        response = requests.get(category_url, headers=HEADERS, timeout=30)
        response.raise_for_status()  # Raise error f√∂r 4xx/5xx responses
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles_metadata = []
        
        for link in soup.find_all('a', class_='hs-kb-category-article-list__link'):
            title = link.get_text(strip=True)
            url = link.get('href')
            slug = create_slug(title)
            
            articles_metadata.append({
                'title': title,
                'url': url,
                'slug': slug
            })
        
        logger.info(f"‚úì Hittade {len(articles_metadata)} artiklar i listan")
        return articles_metadata
        
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 429:
            logger.error(f"‚ùå Rate limited (429)! V√§ntar 60 sekunder...")
            time.sleep(60)
        else:
            logger.error(f"‚ùå HTTP error {e.response.status_code}: {e}")
        return []
    except requests.exceptions.Timeout:
        logger.error(f"‚ùå Timeout vid h√§mtning av {category_url}")
        return []
    except Exception as e:
        logger.error(f"‚ùå Ov√§ntat fel vid h√§mtning av artikellista: {e}")
        return []

def scrape_full_article(url):
    """Skrapar fullst√§ndig artikel fr√•n URL"""
    try:
        logger.info(f"Skrapar artikel: {url}")
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # H√§mta titel
        title_elem = soup.find('h1')
        title = title_elem.get_text(strip=True) if title_elem else 'Ingen titel'
        
        # H√§mta artikel
        article = soup.find('article')
        if not article:
            return None
        
        # Ta bort navigering, sidebar, feedback
        for elem in article.find_all(['aside', 'nav']):
            elem.decompose()
        for elem in article.find_all(class_=['hs-kb-sidebar', 'hs-kb-breadcrumbs', 'hs-kb-social_follow']):
            elem.decompose()
        for elem in article.find_all(string=lambda text: text and 'Hj√§lpte artikeln' in text):
            if elem.parent:
                elem.parent.decompose()
        
        # H√§mta text
        text = article.get_text(separator='\n', strip=True)
        
        # Rensa whitespace
        text = re.sub(r'\n\n\n+', '\n\n', text)
        
        # Hitta datum
        date_match = re.search(r'den \d+ \w+ \d{4}', text)
        date = date_match.group(0) if date_match else ''
        
        # Ta bort datum fr√•n text
        if date:
            text = text.replace(date, '', 1).strip()
        
        # Ta bort titel fr√•n b√∂rjan om den upprepas
        if text.startswith(title):
            text = text[len(title):].strip()
        
        return {
            'title': title,
            'date': date,
            'content': text,
            'url': url
        }
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 429:
            logger.error(f"‚ùå Rate limited (429) vid scraping av {url}! V√§ntar 60 sekunder...")
            time.sleep(60)
        else:
            logger.error(f"‚ùå HTTP error {e.response.status_code} f√∂r {url}")
        return None
    except requests.exceptions.Timeout:
        logger.error(f"‚ùå Timeout vid scraping av {url}")
        return None
    except Exception as e:
        logger.error(f"‚ùå Ov√§ntat fel vid scraping av {url}: {e}")
        return None

def incremental_scrape():
    """
    Inkrementell scraping - bara nya, √§ndrade och raderade
    
    Returns:
        dict: Statistik √∂ver √§ndringar
    """
    logger.info("="*70)
    logger.info("üîÑ INKREMENTELL SCRAPING STARTAR")
    logger.info("="*70)
    logger.info("ü§ñ User-Agent: AxelssonDocBot/1.0")
    logger.info("‚è∞ Delay mellan requests: 1 sekund")
    logger.info("üåê K√§lla: knowledge.flexapplications.se")
    logger.info("‚úÖ Respekterar robots.txt")
    logger.info("="*70)
    
    stats = {
        'new': 0,
        'updated': 0,
        'deleted': 0,
        'unchanged': 0,
        'errors': 0
    }
    
    for category in CATEGORIES:
        logger.info(f"\nüìÇ Kategori: {category['name']}")
        folder = category['folder']
        
        # 1. Ladda befintliga artiklar
        existing_articles = load_existing_articles(folder)
        logger.info(f"  üìÑ Befintliga artiklar: {len(existing_articles)}")
        
        # 2. H√§mta artikellista fr√•n k√§llan
        source_articles = get_article_metadata_from_source(category['url'])
        
        if not source_articles:
            logger.warning(f"  ‚ö†Ô∏è  Kunde inte h√§mta artiklar fr√•n k√§llan, hoppar √∂ver kategori")
            stats['errors'] += 1
            continue
        
        # 3. Skapa lookup f√∂r snabb s√∂kning
        source_slugs = {article['slug']: article for article in source_articles}
        
        # 4. Hitta nya och uppdaterade artiklar
        for source_article in source_articles:
            slug = source_article['slug']
            
            if slug not in existing_articles:
                # NY ARTIKEL
                logger.info(f"  üÜï NY: {source_article['title'][:60]}")
                
                # Skrapa hela artikeln
                full_article = scrape_full_article(source_article['url'])
                
                if full_article:
                    # TODO: Spara artikel (anv√§nd befintlig save_article funktion)
                    stats['new'] += 1
                    logger.info(f"     ‚úì Sparad")
                else:
                    stats['errors'] += 1
                    logger.error(f"     ‚ùå Misslyckades")
                
                time.sleep(1)  # Respektera servern - 1 sekund delay
                
            else:
                # Artikel finns - kontrollera om uppdaterad
                existing = existing_articles[slug]
                
                # Kolla om titel √§ndrats (indikerar stor uppdatering)
                title_changed = existing['title'] != source_article['title']
                
                # F√∂r att verkligen veta om artikeln √§ndrats m√•ste vi h√§mta den och kolla datum
                # Men f√∂r att inte skrapa ALLA artiklar varje g√•ng, g√∂r vi bara stickprov
                # Alternativ: Implementera detta i framtiden med HEAD requests
                
                if title_changed:
                    logger.info(f"  ‚úèÔ∏è  UPPDATERAD: {source_article['title'][:60]}")
                    
                    # Skrapa hela artikeln
                    full_article = scrape_full_article(source_article['url'])
                    
                    if full_article:
                        # J√§mf√∂r datum f√∂r att s√§kerst√§lla att det verkligen √§ndrats
                        if full_article['date'] != existing.get('date'):
                            stats['updated'] += 1
                            logger.info(f"     ‚úì Uppdaterad")
                            # TODO: Uppdatera artikel
                        else:
                            stats['unchanged'] += 1
                    else:
                        stats['errors'] += 1
                        logger.error(f"     ‚ùå Misslyckades")
                    
                    time.sleep(1)  # Respektera servern
                else:
                    # Of√∂r√§ndrad
                    stats['unchanged'] += 1
        
        # 5. Hitta raderade artiklar
        for slug in existing_articles:
            if slug not in source_slugs:
                article = existing_articles[slug]
                logger.info(f"  ‚ùå RADERAD: {article['title'][:60]}")
                stats['deleted'] += 1
                # TODO: Ta bort artikel-fil och ta bort fr√•n index
    
    # Sammanfattning
    logger.info("\n" + "="*70)
    logger.info("üìä SAMMANFATTNING")
    logger.info("="*70)
    logger.info(f"  üÜï Nya artiklar:        {stats['new']}")
    logger.info(f"  ‚úèÔ∏è  Uppdaterade artiklar: {stats['updated']}")
    logger.info(f"  ‚ùå Raderade artiklar:    {stats['deleted']}")
    logger.info(f"  ‚úì  Of√∂r√§ndrade artiklar: {stats['unchanged']}")
    logger.info(f"  ‚ö†Ô∏è  Fel:                 {stats['errors']}")
    logger.info("="*70)
    
    # Output f√∂r parsing
    total_changes = stats['new'] + stats['updated'] + stats['deleted']
    if total_changes > 0:
        parts = []
        if stats['new'] > 0:
            parts.append(f"{stats['new']} nya")
        if stats['updated'] > 0:
            parts.append(f"{stats['updated']} uppdaterade")
        if stats['deleted'] > 0:
            parts.append(f"{stats['deleted']} raderade")
        logger.info(f"\n‚úì {', '.join(parts)}")
    else:
        logger.info("\n‚ÑπÔ∏è  Inga √§ndringar detekterades")
    
    return stats

if __name__ == '__main__':
    import sys
    
    # Kolla om --incremental flagga finns
    if '--incremental' in sys.argv or '-i' in sys.argv:
        stats = incremental_scrape()
        
        # Exit code baserat p√• resultat
        if stats['errors'] > 0:
            sys.exit(1)  # Fel uppstod
        else:
            sys.exit(0)  # Success
    else:
        print("‚ö†Ô∏è  Denna scraper k√∂r endast i inkrementellt l√§ge")
        print("Anv√§ndning: python3 scraper_incremental.py --incremental")
        print("\nF√∂r full scraping, anv√§nd: python3 scraper.py")
        sys.exit(1)

